{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = pd.read_excel(\"THA2train.xlsx\")\n",
    "validate_data = pd.read_excel(\"THA2validate.xlsx\")\n",
    "\n",
    "# Extract features and labels\n",
    "train_X = train_data.iloc[:, :-1].values\n",
    "train_y = train_data.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "validate_X = validate_data.iloc[:, :-1].values\n",
    "validate_y = validate_data.iloc[:, -1].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features (Preprocessing step)\n",
    "train_X = (train_X - np.mean(train_X, axis=0)) / np.std(train_X, axis=0)\n",
    "validate_X = (validate_X - np.mean(validate_X, axis=0)) / np.std(validate_X, axis=0)\n",
    "\n",
    "# One-hot encode labels to represent categorical variables\n",
    "train_y_one_hot = np.eye(2)[train_y.flatten()]\n",
    "validate_y_one_hot = np.eye(2)[validate_y.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "# ReLU is widely used in hidden layers as it allows the model to learn complex patterns and representations.\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "# The Sigmoid function is commonly used in the output layer of binary classification models\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "def binary_cross_entropy_loss(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 10\n",
    "output_size = 2\n",
    "\n",
    "# Initialize parameters to zero\n",
    "weights_0 = np.zeros((input_size, hidden_size))\n",
    "bias_0 = np.zeros((1, hidden_size))\n",
    "\n",
    "weights_1 = np.zeros((hidden_size, hidden_size))\n",
    "bias_1 = np.zeros((1, hidden_size))\n",
    "\n",
    "weights_2 = np.zeros((hidden_size, output_size))\n",
    "bias_2 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 20\n",
    "best_loss = np.inf\n",
    "early_stop_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.6931592092356419, Validation Loss: 0.6931471805600078\n",
      "Epoch: 1, Training Loss: 0.6931591828210342, Validation Loss: 0.6931471805601868\n",
      "Epoch: 2, Training Loss: 0.693159157240238, Validation Loss: 0.693147180560472\n",
      "Epoch: 3, Training Loss: 0.6931591324669288, Validation Loss: 0.6931471805608526\n",
      "Epoch: 4, Training Loss: 0.6931591084756146, Validation Loss: 0.6931471805613191\n",
      "Epoch: 5, Training Loss: 0.6931590852416082, Validation Loss: 0.6931471805618634\n",
      "Epoch: 6, Training Loss: 0.6931590627410009, Validation Loss: 0.6931471805624763\n",
      "Epoch: 7, Training Loss: 0.6931590409506405, Validation Loss: 0.6931471805631508\n",
      "Epoch: 8, Training Loss: 0.6931590198481046, Validation Loss: 0.69314718056388\n",
      "Epoch: 9, Training Loss: 0.6931589994116797, Validation Loss: 0.6931471805646573\n",
      "Epoch: 10, Training Loss: 0.6931589796203371, Validation Loss: 0.6931471805654769\n",
      "Epoch: 11, Training Loss: 0.6931589604537127, Validation Loss: 0.6931471805663331\n",
      "Epoch: 12, Training Loss: 0.6931589418920854, Validation Loss: 0.693147180567221\n",
      "Epoch: 13, Training Loss: 0.6931589239163561, Validation Loss: 0.6931471805681358\n",
      "Epoch: 14, Training Loss: 0.6931589065080298, Validation Loss: 0.6931471805690735\n",
      "Epoch: 15, Training Loss: 0.6931588896491946, Validation Loss: 0.69314718057003\n",
      "Epoch: 16, Training Loss: 0.6931588733225044, Validation Loss: 0.6931471805710016\n",
      "Epoch: 17, Training Loss: 0.693158857511161, Validation Loss: 0.6931471805719854\n",
      "Epoch: 18, Training Loss: 0.6931588421988959, Validation Loss: 0.693147180572978\n",
      "Epoch: 19, Training Loss: 0.6931588273699549, Validation Loss: 0.6931471805739765\n",
      "Epoch: 20, Training Loss: 0.6931588130090813, Validation Loss: 0.693147180574979\n",
      "Epoch: 21, Training Loss: 0.6931587991014992, Validation Loss: 0.6931471805759828\n",
      "Epoch: 22, Training Loss: 0.6931587856329001, Validation Loss: 0.6931471805769857\n",
      "Epoch: 23, Training Loss: 0.6931587725894267, Validation Loss: 0.6931471805779859\n",
      "Epoch: 24, Training Loss: 0.6931587599576592, Validation Loss: 0.6931471805789817\n",
      "Epoch: 25, Training Loss: 0.693158747724602, Validation Loss: 0.6931471805799715\n",
      "Epoch: 26, Training Loss: 0.693158735877669, Validation Loss: 0.6931471805809541\n",
      "Epoch: 27, Training Loss: 0.6931587244046722, Validation Loss: 0.6931471805819281\n",
      "Epoch: 28, Training Loss: 0.6931587132938078, Validation Loss: 0.6931471805828924\n",
      "Epoch: 29, Training Loss: 0.6931587025336449, Validation Loss: 0.693147180583846\n",
      "Epoch: 30, Training Loss: 0.6931586921131132, Validation Loss: 0.693147180584788\n",
      "Epoch: 31, Training Loss: 0.6931586820214929, Validation Loss: 0.6931471805857176\n",
      "Epoch: 32, Training Loss: 0.6931586722484007, Validation Loss: 0.6931471805866339\n",
      "Epoch: 33, Training Loss: 0.6931586627837825, Validation Loss: 0.693147180587537\n",
      "Epoch: 34, Training Loss: 0.6931586536179016, Validation Loss: 0.6931471805884257\n",
      "Epoch: 35, Training Loss: 0.6931586447413286, Validation Loss: 0.6931471805892999\n",
      "Epoch: 36, Training Loss: 0.6931586361449307, Validation Loss: 0.6931471805901589\n",
      "Epoch: 37, Training Loss: 0.6931586278198646, Validation Loss: 0.6931471805910024\n",
      "Epoch: 38, Training Loss: 0.6931586197575661, Validation Loss: 0.6931471805918308\n",
      "Epoch: 39, Training Loss: 0.6931586119497403, Validation Loss: 0.6931471805926434\n",
      "Epoch: 40, Training Loss: 0.6931586043883556, Validation Loss: 0.6931471805934399\n",
      "Epoch: 41, Training Loss: 0.6931585970656328, Validation Loss: 0.6931471805942204\n",
      "Epoch: 42, Training Loss: 0.6931585899740387, Validation Loss: 0.6931471805949849\n",
      "Epoch: 43, Training Loss: 0.6931585831062781, Validation Loss: 0.6931471805957332\n",
      "Epoch: 44, Training Loss: 0.6931585764552856, Validation Loss: 0.6931471805964655\n",
      "Epoch: 45, Training Loss: 0.6931585700142191, Validation Loss: 0.6931471805971818\n",
      "Epoch: 46, Training Loss: 0.6931585637764525, Validation Loss: 0.6931471805978822\n",
      "Epoch: 47, Training Loss: 0.693158557735569, Validation Loss: 0.6931471805985665\n",
      "Epoch: 48, Training Loss: 0.6931585518853538, Validation Loss: 0.6931471805992352\n",
      "Epoch: 49, Training Loss: 0.6931585462197891, Validation Loss: 0.6931471805998881\n",
      "Epoch: 50, Training Loss: 0.6931585407330463, Validation Loss: 0.6931471806005257\n",
      "Epoch: 51, Training Loss: 0.6931585354194814, Validation Loss: 0.693147180601148\n",
      "Epoch: 52, Training Loss: 0.6931585302736278, Validation Loss: 0.693147180601755\n",
      "Epoch: 53, Training Loss: 0.6931585252901923, Validation Loss: 0.6931471806023471\n",
      "Epoch: 54, Training Loss: 0.6931585204640479, Validation Loss: 0.6931471806029247\n",
      "Epoch: 55, Training Loss: 0.6931585157902307, Validation Loss: 0.6931471806034877\n",
      "Epoch: 56, Training Loss: 0.6931585112639321, Validation Loss: 0.6931471806040364\n",
      "Epoch: 57, Training Loss: 0.6931585068804961, Validation Loss: 0.6931471806045708\n",
      "Epoch: 58, Training Loss: 0.693158502635413, Validation Loss: 0.6931471806050917\n",
      "Epoch: 59, Training Loss: 0.6931584985243169, Validation Loss: 0.6931471806055989\n",
      "Epoch: 60, Training Loss: 0.6931584945429778, Validation Loss: 0.6931471806060929\n",
      "Epoch: 61, Training Loss: 0.6931584906873008, Validation Loss: 0.6931471806065739\n",
      "Epoch: 62, Training Loss: 0.6931584869533189, Validation Loss: 0.693147180607042\n",
      "Epoch: 63, Training Loss: 0.6931584833371918, Validation Loss: 0.6931471806074976\n",
      "Epoch: 64, Training Loss: 0.6931584798351988, Validation Loss: 0.6931471806079408\n",
      "Epoch: 65, Training Loss: 0.6931584764437377, Validation Loss: 0.6931471806083719\n",
      "Epoch: 66, Training Loss: 0.6931584731593206, Validation Loss: 0.6931471806087914\n",
      "Epoch: 67, Training Loss: 0.6931584699785679, Validation Loss: 0.6931471806091996\n",
      "Epoch: 68, Training Loss: 0.6931584668982075, Validation Loss: 0.6931471806095959\n",
      "Epoch: 69, Training Loss: 0.6931584639150716, Validation Loss: 0.6931471806099817\n",
      "Epoch: 70, Training Loss: 0.6931584610260906, Validation Loss: 0.6931471806103566\n",
      "Epoch: 71, Training Loss: 0.6931584582282936, Validation Loss: 0.6931471806107212\n",
      "Epoch: 72, Training Loss: 0.6931584555188015, Validation Loss: 0.6931471806110753\n",
      "Epoch: 73, Training Loss: 0.6931584528948284, Validation Loss: 0.6931471806114194\n",
      "Epoch: 74, Training Loss: 0.6931584503536736, Validation Loss: 0.6931471806117537\n",
      "Epoch: 75, Training Loss: 0.6931584478927243, Validation Loss: 0.6931471806120787\n",
      "Epoch: 76, Training Loss: 0.6931584455094485, Validation Loss: 0.6931471806123942\n",
      "Epoch: 77, Training Loss: 0.6931584432013947, Validation Loss: 0.693147180612701\n",
      "Epoch: 78, Training Loss: 0.6931584409661891, Validation Loss: 0.6931471806129987\n",
      "Epoch: 79, Training Loss: 0.6931584388015313, Validation Loss: 0.6931471806132878\n",
      "Epoch: 80, Training Loss: 0.6931584367051957, Validation Loss: 0.6931471806135685\n",
      "Epoch: 81, Training Loss: 0.6931584346750254, Validation Loss: 0.6931471806138412\n",
      "Epoch: 82, Training Loss: 0.6931584327089321, Validation Loss: 0.6931471806141056\n",
      "Epoch: 83, Training Loss: 0.6931584308048931, Validation Loss: 0.6931471806143626\n",
      "Epoch: 84, Training Loss: 0.6931584289609505, Validation Loss: 0.6931471806146119\n",
      "Epoch: 85, Training Loss: 0.6931584271752069, Validation Loss: 0.693147180614854\n",
      "Epoch: 86, Training Loss: 0.6931584254458254, Validation Loss: 0.6931471806150891\n",
      "Epoch: 87, Training Loss: 0.6931584237710273, Validation Loss: 0.6931471806153169\n",
      "Epoch: 88, Training Loss: 0.6931584221490895, Validation Loss: 0.6931471806155381\n",
      "Epoch: 89, Training Loss: 0.6931584205783442, Validation Loss: 0.6931471806157529\n",
      "Epoch: 90, Training Loss: 0.693158419057175, Validation Loss: 0.6931471806159613\n",
      "Epoch: 91, Training Loss: 0.6931584175840173, Validation Loss: 0.6931471806161632\n",
      "Epoch: 92, Training Loss: 0.6931584161573561, Validation Loss: 0.6931471806163594\n",
      "Epoch: 93, Training Loss: 0.6931584147757235, Validation Loss: 0.6931471806165496\n",
      "Epoch: 94, Training Loss: 0.6931584134376982, Validation Loss: 0.693147180616734\n",
      "Epoch: 95, Training Loss: 0.6931584121419044, Validation Loss: 0.6931471806169127\n",
      "Epoch: 96, Training Loss: 0.6931584108870082, Validation Loss: 0.6931471806170865\n",
      "Epoch: 97, Training Loss: 0.6931584096717198, Validation Loss: 0.693147180617255\n",
      "Epoch: 98, Training Loss: 0.6931584084947884, Validation Loss: 0.6931471806174182\n",
      "Epoch: 99, Training Loss: 0.6931584073550037, Validation Loss: 0.6931471806175767\n",
      "Epoch: 100, Training Loss: 0.6931584062511928, Validation Loss: 0.6931471806177301\n",
      "Epoch: 101, Training Loss: 0.6931584051822212, Validation Loss: 0.6931471806178789\n",
      "Epoch: 102, Training Loss: 0.6931584041469885, Validation Loss: 0.6931471806180235\n",
      "Epoch: 103, Training Loss: 0.6931584031444301, Validation Loss: 0.6931471806181635\n",
      "Epoch: 104, Training Loss: 0.6931584021735145, Validation Loss: 0.6931471806182992\n",
      "Epoch: 105, Training Loss: 0.6931584012332432, Validation Loss: 0.6931471806184307\n",
      "Epoch: 106, Training Loss: 0.6931584003226489, Validation Loss: 0.6931471806185584\n",
      "Epoch: 107, Training Loss: 0.693158399440795, Validation Loss: 0.693147180618682\n",
      "Epoch: 108, Training Loss: 0.6931583985867745, Validation Loss: 0.693147180618802\n",
      "Epoch: 109, Training Loss: 0.6931583977597087, Validation Loss: 0.6931471806189181\n",
      "Epoch: 110, Training Loss: 0.6931583969587466, Validation Loss: 0.6931471806190308\n",
      "Epoch: 111, Training Loss: 0.693158396183065, Validation Loss: 0.6931471806191402\n",
      "Epoch: 112, Training Loss: 0.6931583954318653, Validation Loss: 0.693147180619246\n",
      "Epoch: 113, Training Loss: 0.6931583947043755, Validation Loss: 0.6931471806193485\n",
      "Epoch: 114, Training Loss: 0.6931583939998465, Validation Loss: 0.6931471806194484\n",
      "Epoch: 115, Training Loss: 0.6931583933175539, Validation Loss: 0.6931471806195446\n",
      "Epoch: 116, Training Loss: 0.6931583926567959, Validation Loss: 0.6931471806196381\n",
      "Epoch: 117, Training Loss: 0.6931583920168932, Validation Loss: 0.6931471806197287\n",
      "Epoch: 118, Training Loss: 0.6931583913971872, Validation Loss: 0.6931471806198165\n",
      "Epoch: 119, Training Loss: 0.6931583907970401, Validation Loss: 0.6931471806199015\n",
      "Epoch: 120, Training Loss: 0.693158390215835, Validation Loss: 0.6931471806199839\n",
      "Epoch: 121, Training Loss: 0.6931583896529744, Validation Loss: 0.6931471806200636\n",
      "Epoch: 122, Training Loss: 0.6931583891078783, Validation Loss: 0.6931471806201412\n",
      "Epoch: 123, Training Loss: 0.6931583885799874, Validation Loss: 0.6931471806202163\n",
      "Epoch: 124, Training Loss: 0.6931583880687572, Validation Loss: 0.6931471806202889\n",
      "Epoch: 125, Training Loss: 0.6931583875736624, Validation Loss: 0.6931471806203593\n",
      "Epoch: 126, Training Loss: 0.6931583870941941, Validation Loss: 0.6931471806204276\n",
      "Epoch: 127, Training Loss: 0.6931583866298591, Validation Loss: 0.6931471806204937\n",
      "Epoch: 128, Training Loss: 0.6931583861801792, Validation Loss: 0.6931471806205579\n",
      "Epoch: 129, Training Loss: 0.6931583857446919, Validation Loss: 0.6931471806206199\n",
      "Epoch: 130, Training Loss: 0.6931583853229498, Validation Loss: 0.6931471806206801\n",
      "Epoch: 131, Training Loss: 0.693158384914519, Validation Loss: 0.6931471806207387\n",
      "Epoch: 132, Training Loss: 0.6931583845189789, Validation Loss: 0.6931471806207948\n",
      "Epoch: 133, Training Loss: 0.6931583841359229, Validation Loss: 0.6931471806208498\n",
      "Epoch: 134, Training Loss: 0.6931583837649569, Validation Loss: 0.6931471806209027\n",
      "Epoch: 135, Training Loss: 0.6931583834056994, Validation Loss: 0.693147180620954\n",
      "Epoch: 136, Training Loss: 0.6931583830577809, Validation Loss: 0.6931471806210038\n",
      "Epoch: 137, Training Loss: 0.6931583827208433, Validation Loss: 0.6931471806210521\n",
      "Epoch: 138, Training Loss: 0.6931583823945403, Validation Loss: 0.6931471806210987\n",
      "Epoch: 139, Training Loss: 0.6931583820785361, Validation Loss: 0.6931471806211442\n",
      "Epoch: 140, Training Loss: 0.6931583817725057, Validation Loss: 0.693147180621188\n",
      "Epoch: 141, Training Loss: 0.6931583814761342, Validation Loss: 0.6931471806212305\n",
      "Epoch: 142, Training Loss: 0.6931583811891167, Validation Loss: 0.6931471806212715\n",
      "Epoch: 143, Training Loss: 0.6931583809111583, Validation Loss: 0.6931471806213114\n",
      "Epoch: 144, Training Loss: 0.6931583806419728, Validation Loss: 0.6931471806213501\n",
      "Epoch: 145, Training Loss: 0.6931583803812831, Validation Loss: 0.6931471806213876\n",
      "Epoch: 146, Training Loss: 0.6931583801288215, Validation Loss: 0.6931471806214237\n",
      "Epoch: 147, Training Loss: 0.6931583798843283, Validation Loss: 0.6931471806214589\n",
      "Epoch: 148, Training Loss: 0.6931583796475516, Validation Loss: 0.6931471806214928\n",
      "Epoch: 149, Training Loss: 0.6931583794182482, Validation Loss: 0.6931471806215258\n",
      "Epoch: 150, Training Loss: 0.6931583791961818, Validation Loss: 0.6931471806215577\n",
      "Epoch: 151, Training Loss: 0.6931583789811242, Validation Loss: 0.6931471806215889\n",
      "Epoch: 152, Training Loss: 0.6931583787728546, Validation Loss: 0.6931471806216187\n",
      "Epoch: 153, Training Loss: 0.6931583785711584, Validation Loss: 0.6931471806216477\n",
      "Epoch: 154, Training Loss: 0.6931583783758282, Validation Loss: 0.6931471806216757\n",
      "Epoch: 155, Training Loss: 0.693158378186663, Validation Loss: 0.6931471806217029\n",
      "Epoch: 156, Training Loss: 0.6931583780034682, Validation Loss: 0.6931471806217292\n",
      "Epoch: 157, Training Loss: 0.6931583778260553, Validation Loss: 0.693147180621755\n",
      "Epoch: 158, Training Loss: 0.6931583776542423, Validation Loss: 0.6931471806217796\n",
      "Epoch: 159, Training Loss: 0.6931583774878514, Validation Loss: 0.6931471806218034\n",
      "Epoch: 160, Training Loss: 0.6931583773267126, Validation Loss: 0.693147180621827\n",
      "Epoch: 161, Training Loss: 0.6931583771706595, Validation Loss: 0.6931471806218492\n",
      "Epoch: 162, Training Loss: 0.693158377019532, Validation Loss: 0.693147180621871\n",
      "Epoch: 163, Training Loss: 0.6931583768731743, Validation Loss: 0.6931471806218921\n",
      "Epoch: 164, Training Loss: 0.693158376731436, Validation Loss: 0.6931471806219124\n",
      "Epoch: 165, Training Loss: 0.6931583765941709, Validation Loss: 0.6931471806219321\n",
      "Epoch: 166, Training Loss: 0.6931583764612385, Validation Loss: 0.6931471806219517\n",
      "Epoch: 167, Training Loss: 0.6931583763325015, Validation Loss: 0.6931471806219699\n",
      "Epoch: 168, Training Loss: 0.693158376207828, Validation Loss: 0.6931471806219881\n",
      "Epoch: 169, Training Loss: 0.6931583760870894, Validation Loss: 0.6931471806220054\n",
      "Epoch: 170, Training Loss: 0.6931583759701615, Validation Loss: 0.6931471806220223\n",
      "Epoch: 171, Training Loss: 0.693158375856924, Validation Loss: 0.6931471806220385\n",
      "Epoch: 172, Training Loss: 0.6931583757472602, Validation Loss: 0.6931471806220545\n",
      "Epoch: 173, Training Loss: 0.6931583756410586, Validation Loss: 0.6931471806220698\n",
      "Epoch: 174, Training Loss: 0.6931583755382082, Validation Loss: 0.6931471806220847\n",
      "Epoch: 175, Training Loss: 0.693158375438604, Validation Loss: 0.6931471806220991\n",
      "Epoch: 176, Training Loss: 0.6931583753421435, Validation Loss: 0.6931471806221129\n",
      "Epoch: 177, Training Loss: 0.6931583752487279, Validation Loss: 0.6931471806221265\n",
      "Epoch: 178, Training Loss: 0.6931583751582605, Validation Loss: 0.6931471806221394\n",
      "Epoch: 179, Training Loss: 0.6931583750706483, Validation Loss: 0.6931471806221521\n",
      "Epoch: 180, Training Loss: 0.6931583749858011, Validation Loss: 0.6931471806221642\n",
      "Epoch: 181, Training Loss: 0.6931583749036323, Validation Loss: 0.6931471806221763\n",
      "Epoch: 182, Training Loss: 0.6931583748240567, Validation Loss: 0.6931471806221877\n",
      "Epoch: 183, Training Loss: 0.6931583747469927, Validation Loss: 0.6931471806221988\n",
      "Epoch: 184, Training Loss: 0.6931583746723611, Validation Loss: 0.6931471806222095\n",
      "Epoch: 185, Training Loss: 0.6931583746000849, Validation Loss: 0.69314718062222\n",
      "Epoch: 186, Training Loss: 0.6931583745300897, Validation Loss: 0.6931471806222301\n",
      "Epoch: 187, Training Loss: 0.6931583744623039, Validation Loss: 0.6931471806222401\n",
      "Epoch: 188, Training Loss: 0.693158374396658, Validation Loss: 0.6931471806222494\n",
      "Epoch: 189, Training Loss: 0.6931583743330834, Validation Loss: 0.6931471806222587\n",
      "Epoch: 190, Training Loss: 0.6931583742715157, Validation Loss: 0.6931471806222675\n",
      "Epoch: 191, Training Loss: 0.6931583742118909, Validation Loss: 0.6931471806222763\n",
      "Epoch: 192, Training Loss: 0.693158374154148, Validation Loss: 0.6931471806222846\n",
      "Epoch: 193, Training Loss: 0.6931583740982281, Validation Loss: 0.6931471806222927\n",
      "Epoch: 194, Training Loss: 0.6931583740440724, Validation Loss: 0.6931471806223005\n",
      "Epoch: 195, Training Loss: 0.6931583739916266, Validation Loss: 0.693147180622308\n",
      "Epoch: 196, Training Loss: 0.6931583739408355, Validation Loss: 0.6931471806223154\n",
      "Epoch: 197, Training Loss: 0.6931583738916479, Validation Loss: 0.6931471806223225\n",
      "Epoch: 198, Training Loss: 0.6931583738440128, Validation Loss: 0.6931471806223295\n",
      "Epoch: 199, Training Loss: 0.6931583737978808, Validation Loss: 0.693147180622336\n",
      "Epoch: 200, Training Loss: 0.693158373753205, Validation Loss: 0.6931471806223426\n",
      "Epoch: 201, Training Loss: 0.6931583737099394, Validation Loss: 0.6931471806223489\n",
      "Epoch: 202, Training Loss: 0.6931583736680392, Validation Loss: 0.6931471806223547\n",
      "Epoch: 203, Training Loss: 0.6931583736274614, Validation Loss: 0.6931471806223607\n",
      "Epoch: 204, Training Loss: 0.6931583735881645, Validation Loss: 0.6931471806223662\n",
      "Epoch: 205, Training Loss: 0.6931583735501078, Validation Loss: 0.6931471806223719\n",
      "Epoch: 206, Training Loss: 0.6931583735132522, Validation Loss: 0.6931471806223772\n",
      "Epoch: 207, Training Loss: 0.6931583734775599, Validation Loss: 0.6931471806223825\n",
      "Epoch: 208, Training Loss: 0.6931583734429941, Validation Loss: 0.6931471806223874\n",
      "Epoch: 209, Training Loss: 0.6931583734095195, Validation Loss: 0.6931471806223923\n",
      "Epoch: 210, Training Loss: 0.6931583733771013, Validation Loss: 0.693147180622397\n",
      "Epoch: 211, Training Loss: 0.6931583733457061, Validation Loss: 0.6931471806224014\n",
      "Epoch: 212, Training Loss: 0.6931583733153015, Validation Loss: 0.693147180622406\n",
      "Epoch: 213, Training Loss: 0.6931583732858573, Validation Loss: 0.6931471806224102\n",
      "Epoch: 214, Training Loss: 0.6931583732573418, Validation Loss: 0.6931471806224143\n",
      "Epoch: 215, Training Loss: 0.6931583732297266, Validation Loss: 0.6931471806224182\n",
      "Epoch: 216, Training Loss: 0.6931583732029831, Validation Loss: 0.6931471806224222\n",
      "Epoch: 217, Training Loss: 0.6931583731770835, Validation Loss: 0.6931471806224259\n",
      "Epoch: 218, Training Loss: 0.6931583731520015, Validation Loss: 0.6931471806224295\n",
      "Epoch: 219, Training Loss: 0.6931583731277109, Validation Loss: 0.6931471806224332\n",
      "Epoch: 220, Training Loss: 0.6931583731041868, Validation Loss: 0.6931471806224366\n",
      "Epoch: 221, Training Loss: 0.6931583730814056, Validation Loss: 0.6931471806224397\n",
      "Epoch: 222, Training Loss: 0.6931583730593432, Validation Loss: 0.693147180622443\n",
      "Epoch: 223, Training Loss: 0.6931583730379776, Validation Loss: 0.6931471806224461\n",
      "Epoch: 224, Training Loss: 0.6931583730172857, Validation Loss: 0.6931471806224491\n",
      "Epoch: 225, Training Loss: 0.6931583729972473, Validation Loss: 0.6931471806224518\n",
      "Epoch: 226, Training Loss: 0.693158372977841, Validation Loss: 0.693147180622455\n",
      "Epoch: 227, Training Loss: 0.6931583729590474, Validation Loss: 0.6931471806224574\n",
      "Epoch: 228, Training Loss: 0.6931583729408468, Validation Loss: 0.6931471806224601\n",
      "Epoch: 229, Training Loss: 0.693158372923221, Validation Loss: 0.6931471806224627\n",
      "Epoch: 230, Training Loss: 0.6931583729061512, Validation Loss: 0.6931471806224653\n",
      "Epoch: 231, Training Loss: 0.6931583728896202, Validation Loss: 0.6931471806224676\n",
      "Epoch: 232, Training Loss: 0.6931583728736115, Validation Loss: 0.6931471806224699\n",
      "Epoch: 233, Training Loss: 0.6931583728581072, Validation Loss: 0.6931471806224719\n",
      "Epoch: 234, Training Loss: 0.6931583728430926, Validation Loss: 0.6931471806224743\n",
      "Epoch: 235, Training Loss: 0.6931583728285519, Validation Loss: 0.6931471806224765\n",
      "Epoch: 236, Training Loss: 0.69315837281447, Validation Loss: 0.6931471806224784\n",
      "Epoch: 237, Training Loss: 0.6931583728008328, Validation Loss: 0.6931471806224805\n",
      "Epoch: 238, Training Loss: 0.6931583727876259, Validation Loss: 0.6931471806224824\n",
      "Epoch: 239, Training Loss: 0.6931583727748356, Validation Loss: 0.6931471806224843\n",
      "Epoch: 240, Training Loss: 0.6931583727624495, Validation Loss: 0.6931471806224859\n",
      "Epoch: 241, Training Loss: 0.6931583727504541, Validation Loss: 0.6931471806224875\n",
      "Epoch: 242, Training Loss: 0.6931583727388374, Validation Loss: 0.6931471806224895\n",
      "Epoch: 243, Training Loss: 0.6931583727275871, Validation Loss: 0.6931471806224911\n",
      "Epoch: 244, Training Loss: 0.6931583727166917, Validation Loss: 0.6931471806224927\n",
      "Epoch: 245, Training Loss: 0.6931583727061409, Validation Loss: 0.693147180622494\n",
      "Epoch: 246, Training Loss: 0.6931583726959227, Validation Loss: 0.6931471806224955\n",
      "Epoch: 247, Training Loss: 0.6931583726860269, Validation Loss: 0.6931471806224969\n",
      "Epoch: 248, Training Loss: 0.6931583726764434, Validation Loss: 0.6931471806224985\n",
      "Epoch: 249, Training Loss: 0.6931583726671627, Validation Loss: 0.6931471806224998\n",
      "Epoch: 250, Training Loss: 0.6931583726581748, Validation Loss: 0.6931471806225009\n",
      "Epoch: 251, Training Loss: 0.6931583726494706, Validation Loss: 0.6931471806225022\n",
      "Epoch: 252, Training Loss: 0.6931583726410409, Validation Loss: 0.6931471806225036\n",
      "Epoch: 253, Training Loss: 0.6931583726328772, Validation Loss: 0.6931471806225047\n",
      "Epoch: 254, Training Loss: 0.6931583726249714, Validation Loss: 0.6931471806225059\n",
      "Epoch: 255, Training Loss: 0.6931583726173152, Validation Loss: 0.693147180622507\n",
      "Epoch: 256, Training Loss: 0.6931583726099004, Validation Loss: 0.6931471806225081\n",
      "Epoch: 257, Training Loss: 0.6931583726027198, Validation Loss: 0.6931471806225091\n",
      "Epoch: 258, Training Loss: 0.6931583725957655, Validation Loss: 0.6931471806225102\n",
      "Epoch: 259, Training Loss: 0.693158372589031, Validation Loss: 0.6931471806225111\n",
      "Epoch: 260, Training Loss: 0.6931583725825092, Validation Loss: 0.6931471806225118\n",
      "Epoch: 261, Training Loss: 0.6931583725761932, Validation Loss: 0.693147180622513\n",
      "Epoch: 262, Training Loss: 0.6931583725700764, Validation Loss: 0.6931471806225138\n",
      "Epoch: 263, Training Loss: 0.6931583725641526, Validation Loss: 0.6931471806225147\n",
      "Epoch: 264, Training Loss: 0.6931583725584158, Validation Loss: 0.6931471806225155\n",
      "Epoch: 265, Training Loss: 0.6931583725528602, Validation Loss: 0.6931471806225163\n",
      "Epoch: 266, Training Loss: 0.6931583725474797, Validation Loss: 0.6931471806225171\n",
      "Epoch: 267, Training Loss: 0.693158372542269, Validation Loss: 0.6931471806225179\n",
      "Epoch: 268, Training Loss: 0.693158372537223, Validation Loss: 0.6931471806225188\n",
      "Epoch: 269, Training Loss: 0.6931583725323363, Validation Loss: 0.6931471806225193\n",
      "Epoch: 270, Training Loss: 0.6931583725276036, Validation Loss: 0.69314718062252\n",
      "Epoch: 271, Training Loss: 0.6931583725230206, Validation Loss: 0.6931471806225208\n",
      "Epoch: 272, Training Loss: 0.6931583725185821, Validation Loss: 0.6931471806225213\n",
      "Epoch: 273, Training Loss: 0.6931583725142834, Validation Loss: 0.693147180622522\n",
      "Epoch: 274, Training Loss: 0.6931583725101207, Validation Loss: 0.6931471806225225\n",
      "Epoch: 275, Training Loss: 0.6931583725060892, Validation Loss: 0.6931471806225231\n",
      "Epoch: 276, Training Loss: 0.6931583725021848, Validation Loss: 0.6931471806225237\n",
      "Epoch: 277, Training Loss: 0.6931583724984041, Validation Loss: 0.6931471806225243\n",
      "Epoch: 278, Training Loss: 0.6931583724947428, Validation Loss: 0.6931471806225248\n",
      "Epoch: 279, Training Loss: 0.6931583724911966, Validation Loss: 0.6931471806225252\n",
      "Epoch: 280, Training Loss: 0.6931583724877624, Validation Loss: 0.6931471806225257\n",
      "Epoch: 281, Training Loss: 0.6931583724844368, Validation Loss: 0.6931471806225262\n",
      "Epoch: 282, Training Loss: 0.6931583724812159, Validation Loss: 0.6931471806225268\n",
      "Epoch: 283, Training Loss: 0.6931583724780966, Validation Loss: 0.6931471806225272\n",
      "Epoch: 284, Training Loss: 0.693158372475076, Validation Loss: 0.6931471806225277\n",
      "Epoch: 285, Training Loss: 0.6931583724721507, Validation Loss: 0.693147180622528\n",
      "Epoch: 286, Training Loss: 0.6931583724693178, Validation Loss: 0.6931471806225284\n",
      "Epoch: 287, Training Loss: 0.6931583724665743, Validation Loss: 0.6931471806225288\n",
      "Epoch: 288, Training Loss: 0.6931583724639174, Validation Loss: 0.6931471806225292\n",
      "Epoch: 289, Training Loss: 0.6931583724613439, Validation Loss: 0.6931471806225296\n",
      "Epoch: 290, Training Loss: 0.6931583724588521, Validation Loss: 0.6931471806225299\n",
      "Epoch: 291, Training Loss: 0.6931583724564387, Validation Loss: 0.6931471806225304\n",
      "Epoch: 292, Training Loss: 0.6931583724541017, Validation Loss: 0.6931471806225308\n",
      "Epoch: 293, Training Loss: 0.6931583724518382, Validation Loss: 0.693147180622531\n",
      "Epoch: 294, Training Loss: 0.6931583724496465, Validation Loss: 0.6931471806225312\n",
      "Epoch: 295, Training Loss: 0.6931583724475239, Validation Loss: 0.6931471806225317\n",
      "Epoch: 296, Training Loss: 0.693158372445468, Validation Loss: 0.6931471806225319\n",
      "Epoch: 297, Training Loss: 0.6931583724434771, Validation Loss: 0.6931471806225321\n",
      "Epoch: 298, Training Loss: 0.6931583724415493, Validation Loss: 0.6931471806225323\n",
      "Epoch: 299, Training Loss: 0.6931583724396823, Validation Loss: 0.6931471806225328\n",
      "Epoch: 300, Training Loss: 0.6931583724378739, Validation Loss: 0.693147180622533\n",
      "Epoch: 301, Training Loss: 0.6931583724361227, Validation Loss: 0.6931471806225333\n",
      "Epoch: 302, Training Loss: 0.6931583724344269, Validation Loss: 0.6931471806225336\n",
      "Epoch: 303, Training Loss: 0.6931583724327844, Validation Loss: 0.6931471806225338\n",
      "Epoch: 304, Training Loss: 0.6931583724311939, Validation Loss: 0.693147180622534\n",
      "Epoch: 305, Training Loss: 0.6931583724296537, Validation Loss: 0.693147180622534\n",
      "Epoch: 306, Training Loss: 0.6931583724281617, Validation Loss: 0.6931471806225344\n",
      "Epoch: 307, Training Loss: 0.6931583724267173, Validation Loss: 0.6931471806225348\n",
      "Epoch: 308, Training Loss: 0.6931583724253183, Validation Loss: 0.6931471806225348\n",
      "Epoch: 309, Training Loss: 0.6931583724239634, Validation Loss: 0.693147180622535\n",
      "Epoch: 310, Training Loss: 0.6931583724226514, Validation Loss: 0.6931471806225351\n",
      "Epoch: 311, Training Loss: 0.6931583724213807, Validation Loss: 0.6931471806225356\n",
      "Epoch: 312, Training Loss: 0.69315837242015, Validation Loss: 0.6931471806225356\n",
      "Epoch: 313, Training Loss: 0.6931583724189584, Validation Loss: 0.6931471806225357\n",
      "Epoch: 314, Training Loss: 0.6931583724178042, Validation Loss: 0.6931471806225359\n",
      "Epoch: 315, Training Loss: 0.6931583724166863, Validation Loss: 0.6931471806225362\n",
      "Epoch: 316, Training Loss: 0.693158372415604, Validation Loss: 0.6931471806225362\n",
      "Epoch: 317, Training Loss: 0.6931583724145557, Validation Loss: 0.6931471806225364\n",
      "Epoch: 318, Training Loss: 0.6931583724135405, Validation Loss: 0.6931471806225364\n",
      "Epoch: 319, Training Loss: 0.6931583724125573, Validation Loss: 0.6931471806225367\n",
      "Epoch: 320, Training Loss: 0.6931583724116054, Validation Loss: 0.693147180622537\n",
      "Epoch: 321, Training Loss: 0.693158372410683, Validation Loss: 0.6931471806225371\n",
      "Epoch: 322, Training Loss: 0.6931583724097903, Validation Loss: 0.6931471806225371\n",
      "Epoch: 323, Training Loss: 0.6931583724089255, Validation Loss: 0.6931471806225371\n",
      "Epoch: 324, Training Loss: 0.693158372408088, Validation Loss: 0.6931471806225373\n",
      "Epoch: 325, Training Loss: 0.6931583724072768, Validation Loss: 0.6931471806225376\n",
      "Epoch: 326, Training Loss: 0.6931583724064914, Validation Loss: 0.6931471806225376\n",
      "Epoch: 327, Training Loss: 0.6931583724057309, Validation Loss: 0.6931471806225376\n",
      "Epoch: 328, Training Loss: 0.6931583724049943, Validation Loss: 0.6931471806225379\n",
      "Epoch: 329, Training Loss: 0.6931583724042808, Validation Loss: 0.6931471806225379\n",
      "Epoch: 330, Training Loss: 0.69315837240359, Validation Loss: 0.693147180622538\n",
      "Epoch: 331, Training Loss: 0.6931583724029208, Validation Loss: 0.693147180622538\n",
      "Epoch: 332, Training Loss: 0.693158372402273, Validation Loss: 0.6931471806225381\n",
      "Epoch: 333, Training Loss: 0.6931583724016452, Validation Loss: 0.6931471806225383\n",
      "Epoch: 334, Training Loss: 0.6931583724010378, Validation Loss: 0.6931471806225382\n",
      "Epoch: 335, Training Loss: 0.6931583724004491, Validation Loss: 0.6931471806225383\n",
      "Epoch: 336, Training Loss: 0.6931583723998792, Validation Loss: 0.6931471806225386\n",
      "Epoch: 337, Training Loss: 0.6931583723993275, Validation Loss: 0.6931471806225387\n",
      "Epoch: 338, Training Loss: 0.6931583723987926, Validation Loss: 0.6931471806225387\n",
      "Epoch: 339, Training Loss: 0.693158372398275, Validation Loss: 0.6931471806225387\n",
      "Epoch: 340, Training Loss: 0.6931583723977736, Validation Loss: 0.6931471806225389\n",
      "Epoch: 341, Training Loss: 0.6931583723972884, Validation Loss: 0.6931471806225389\n",
      "Epoch: 342, Training Loss: 0.693158372396818, Validation Loss: 0.6931471806225389\n",
      "Epoch: 343, Training Loss: 0.6931583723963628, Validation Loss: 0.6931471806225389\n",
      "Epoch: 344, Training Loss: 0.6931583723959218, Validation Loss: 0.6931471806225391\n",
      "Epoch: 345, Training Loss: 0.6931583723954948, Validation Loss: 0.6931471806225391\n",
      "Epoch: 346, Training Loss: 0.693158372395081, Validation Loss: 0.6931471806225392\n",
      "Epoch: 347, Training Loss: 0.6931583723946806, Validation Loss: 0.6931471806225391\n",
      "Epoch: 348, Training Loss: 0.6931583723942925, Validation Loss: 0.6931471806225394\n",
      "Epoch: 349, Training Loss: 0.6931583723939169, Validation Loss: 0.6931471806225394\n",
      "Epoch: 350, Training Loss: 0.6931583723935533, Validation Loss: 0.6931471806225394\n",
      "Epoch: 351, Training Loss: 0.6931583723932012, Validation Loss: 0.6931471806225394\n",
      "Epoch: 352, Training Loss: 0.6931583723928595, Validation Loss: 0.6931471806225395\n",
      "Epoch: 353, Training Loss: 0.6931583723925295, Validation Loss: 0.6931471806225397\n",
      "Epoch: 354, Training Loss: 0.6931583723922092, Validation Loss: 0.6931471806225397\n",
      "Epoch: 355, Training Loss: 0.6931583723918996, Validation Loss: 0.6931471806225395\n",
      "Epoch: 356, Training Loss: 0.6931583723915993, Validation Loss: 0.6931471806225398\n",
      "Epoch: 357, Training Loss: 0.6931583723913087, Validation Loss: 0.6931471806225398\n",
      "Epoch: 358, Training Loss: 0.6931583723910273, Validation Loss: 0.6931471806225398\n",
      "Epoch: 359, Training Loss: 0.6931583723907548, Validation Loss: 0.69314718062254\n",
      "Epoch: 360, Training Loss: 0.6931583723904908, Validation Loss: 0.6931471806225399\n",
      "Epoch: 361, Training Loss: 0.6931583723902353, Validation Loss: 0.6931471806225399\n",
      "Epoch: 362, Training Loss: 0.6931583723899877, Validation Loss: 0.6931471806225399\n",
      "Epoch: 363, Training Loss: 0.693158372389748, Validation Loss: 0.69314718062254\n",
      "Epoch: 364, Training Loss: 0.6931583723895157, Validation Loss: 0.6931471806225401\n",
      "Epoch: 365, Training Loss: 0.6931583723892908, Validation Loss: 0.6931471806225401\n",
      "Epoch: 366, Training Loss: 0.693158372389073, Validation Loss: 0.6931471806225402\n",
      "Epoch: 367, Training Loss: 0.693158372388862, Validation Loss: 0.6931471806225401\n",
      "Epoch: 368, Training Loss: 0.6931583723886579, Validation Loss: 0.6931471806225401\n",
      "Epoch: 369, Training Loss: 0.69315837238846, Validation Loss: 0.6931471806225402\n",
      "Epoch: 370, Training Loss: 0.6931583723882684, Validation Loss: 0.6931471806225402\n",
      "Epoch: 371, Training Loss: 0.6931583723880829, Validation Loss: 0.6931471806225402\n",
      "Epoch: 372, Training Loss: 0.6931583723879032, Validation Loss: 0.6931471806225402\n",
      "Epoch: 373, Training Loss: 0.6931583723877293, Validation Loss: 0.6931471806225402\n",
      "Epoch: 374, Training Loss: 0.693158372387561, Validation Loss: 0.6931471806225403\n",
      "Epoch: 375, Training Loss: 0.6931583723873977, Validation Loss: 0.6931471806225404\n",
      "Epoch: 376, Training Loss: 0.6931583723872398, Validation Loss: 0.6931471806225403\n",
      "Epoch: 377, Training Loss: 0.6931583723870867, Validation Loss: 0.6931471806225404\n",
      "Epoch: 378, Training Loss: 0.6931583723869384, Validation Loss: 0.6931471806225403\n",
      "Epoch: 379, Training Loss: 0.6931583723867949, Validation Loss: 0.6931471806225404\n",
      "Epoch: 380, Training Loss: 0.693158372386656, Validation Loss: 0.6931471806225404\n",
      "Epoch: 381, Training Loss: 0.6931583723865216, Validation Loss: 0.6931471806225405\n",
      "Epoch: 382, Training Loss: 0.6931583723863912, Validation Loss: 0.6931471806225404\n",
      "Epoch: 383, Training Loss: 0.6931583723862645, Validation Loss: 0.6931471806225407\n",
      "Epoch: 384, Training Loss: 0.6931583723861424, Validation Loss: 0.6931471806225404\n",
      "Epoch: 385, Training Loss: 0.6931583723860242, Validation Loss: 0.6931471806225404\n",
      "Epoch: 386, Training Loss: 0.6931583723859092, Validation Loss: 0.6931471806225403\n",
      "Epoch: 387, Training Loss: 0.6931583723857984, Validation Loss: 0.6931471806225403\n",
      "Epoch: 388, Training Loss: 0.6931583723856908, Validation Loss: 0.6931471806225403\n",
      "Epoch: 389, Training Loss: 0.6931583723855865, Validation Loss: 0.6931471806225407\n",
      "Epoch: 390, Training Loss: 0.6931583723854858, Validation Loss: 0.6931471806225407\n",
      "Epoch: 391, Training Loss: 0.6931583723853881, Validation Loss: 0.6931471806225404\n",
      "Epoch: 392, Training Loss: 0.6931583723852932, Validation Loss: 0.6931471806225404\n",
      "Epoch: 393, Training Loss: 0.693158372385202, Validation Loss: 0.6931471806225407\n",
      "Epoch: 394, Training Loss: 0.6931583723851134, Validation Loss: 0.6931471806225407\n",
      "Epoch: 395, Training Loss: 0.6931583723850274, Validation Loss: 0.6931471806225407\n",
      "Epoch: 396, Training Loss: 0.693158372384944, Validation Loss: 0.6931471806225407\n",
      "Epoch: 397, Training Loss: 0.6931583723848636, Validation Loss: 0.6931471806225407\n",
      "Epoch: 398, Training Loss: 0.6931583723847854, Validation Loss: 0.6931471806225407\n",
      "Epoch: 399, Training Loss: 0.6931583723847097, Validation Loss: 0.6931471806225407\n",
      "Epoch: 400, Training Loss: 0.6931583723846367, Validation Loss: 0.6931471806225407\n",
      "Epoch: 401, Training Loss: 0.6931583723845657, Validation Loss: 0.6931471806225408\n",
      "Epoch: 402, Training Loss: 0.6931583723844971, Validation Loss: 0.6931471806225407\n",
      "Epoch: 403, Training Loss: 0.693158372384431, Validation Loss: 0.6931471806225408\n",
      "Epoch: 404, Training Loss: 0.6931583723843662, Validation Loss: 0.6931471806225407\n",
      "Epoch: 405, Training Loss: 0.6931583723843039, Validation Loss: 0.6931471806225408\n",
      "Epoch: 406, Training Loss: 0.6931583723842436, Validation Loss: 0.6931471806225407\n",
      "Epoch: 407, Training Loss: 0.693158372384185, Validation Loss: 0.6931471806225407\n",
      "Epoch: 408, Training Loss: 0.6931583723841287, Validation Loss: 0.6931471806225407\n",
      "Epoch: 409, Training Loss: 0.6931583723840737, Validation Loss: 0.6931471806225407\n",
      "Epoch: 410, Training Loss: 0.6931583723840208, Validation Loss: 0.6931471806225407\n",
      "Epoch: 411, Training Loss: 0.6931583723839694, Validation Loss: 0.6931471806225409\n",
      "Epoch: 412, Training Loss: 0.6931583723839196, Validation Loss: 0.6931471806225408\n",
      "Epoch: 413, Training Loss: 0.6931583723838711, Validation Loss: 0.693147180622541\n",
      "Epoch: 414, Training Loss: 0.6931583723838243, Validation Loss: 0.6931471806225409\n",
      "Epoch: 415, Training Loss: 0.6931583723837792, Validation Loss: 0.6931471806225408\n",
      "Epoch: 416, Training Loss: 0.6931583723837355, Validation Loss: 0.693147180622541\n",
      "Epoch: 417, Training Loss: 0.6931583723836928, Validation Loss: 0.6931471806225408\n",
      "Epoch: 418, Training Loss: 0.6931583723836519, Validation Loss: 0.6931471806225408\n",
      "Epoch: 419, Training Loss: 0.6931583723836122, Validation Loss: 0.6931471806225409\n",
      "Epoch: 420, Training Loss: 0.6931583723835734, Validation Loss: 0.6931471806225409\n",
      "Epoch: 421, Training Loss: 0.6931583723835362, Validation Loss: 0.6931471806225409\n",
      "Epoch: 422, Training Loss: 0.6931583723835, Validation Loss: 0.693147180622541\n",
      "Epoch: 423, Training Loss: 0.6931583723834651, Validation Loss: 0.6931471806225411\n",
      "Epoch: 424, Training Loss: 0.6931583723834313, Validation Loss: 0.693147180622541\n",
      "Epoch: 425, Training Loss: 0.6931583723833982, Validation Loss: 0.6931471806225408\n",
      "Epoch: 426, Training Loss: 0.6931583723833665, Validation Loss: 0.6931471806225408\n",
      "Epoch: 427, Training Loss: 0.6931583723833358, Validation Loss: 0.6931471806225408\n",
      "Epoch: 428, Training Loss: 0.693158372383306, Validation Loss: 0.693147180622541\n",
      "Epoch: 429, Training Loss: 0.6931583723832772, Validation Loss: 0.6931471806225408\n",
      "Epoch: 430, Training Loss: 0.693158372383249, Validation Loss: 0.6931471806225408\n",
      "Epoch: 431, Training Loss: 0.6931583723832221, Validation Loss: 0.693147180622541\n",
      "Epoch: 432, Training Loss: 0.6931583723831959, Validation Loss: 0.693147180622541\n",
      "Epoch: 433, Training Loss: 0.6931583723831705, Validation Loss: 0.6931471806225411\n",
      "Epoch: 434, Training Loss: 0.6931583723831458, Validation Loss: 0.6931471806225409\n",
      "Epoch: 435, Training Loss: 0.6931583723831218, Validation Loss: 0.6931471806225411\n",
      "Epoch: 436, Training Loss: 0.6931583723830991, Validation Loss: 0.693147180622541\n",
      "Epoch: 437, Training Loss: 0.6931583723830763, Validation Loss: 0.6931471806225411\n",
      "Epoch: 438, Training Loss: 0.693158372383055, Validation Loss: 0.6931471806225408\n",
      "Epoch: 439, Training Loss: 0.693158372383034, Validation Loss: 0.6931471806225409\n",
      "Epoch: 440, Training Loss: 0.6931583723830135, Validation Loss: 0.6931471806225408\n",
      "Epoch: 441, Training Loss: 0.6931583723829942, Validation Loss: 0.693147180622541\n",
      "Epoch: 442, Training Loss: 0.6931583723829752, Validation Loss: 0.6931471806225409\n",
      "Epoch: 443, Training Loss: 0.6931583723829563, Validation Loss: 0.693147180622541\n",
      "Epoch: 444, Training Loss: 0.6931583723829389, Validation Loss: 0.693147180622541\n",
      "Epoch: 445, Training Loss: 0.6931583723829214, Validation Loss: 0.6931471806225408\n",
      "Epoch: 446, Training Loss: 0.6931583723829043, Validation Loss: 0.6931471806225409\n",
      "Epoch: 447, Training Loss: 0.6931583723828887, Validation Loss: 0.693147180622541\n",
      "Epoch: 448, Training Loss: 0.6931583723828727, Validation Loss: 0.6931471806225411\n",
      "Epoch: 449, Training Loss: 0.6931583723828574, Validation Loss: 0.693147180622541\n",
      "Epoch: 450, Training Loss: 0.693158372382843, Validation Loss: 0.6931471806225411\n",
      "Epoch: 451, Training Loss: 0.6931583723828284, Validation Loss: 0.6931471806225411\n",
      "Epoch: 452, Training Loss: 0.6931583723828147, Validation Loss: 0.6931471806225411\n",
      "Epoch: 453, Training Loss: 0.6931583723828016, Validation Loss: 0.693147180622541\n",
      "Epoch: 454, Training Loss: 0.6931583723827882, Validation Loss: 0.6931471806225411\n",
      "Epoch: 455, Training Loss: 0.6931583723827757, Validation Loss: 0.6931471806225411\n",
      "Epoch: 456, Training Loss: 0.6931583723827637, Validation Loss: 0.6931471806225411\n",
      "Epoch: 457, Training Loss: 0.6931583723827521, Validation Loss: 0.6931471806225408\n",
      "Epoch: 458, Training Loss: 0.6931583723827407, Validation Loss: 0.6931471806225409\n",
      "Epoch: 459, Training Loss: 0.6931583723827297, Validation Loss: 0.6931471806225409\n",
      "Epoch: 460, Training Loss: 0.6931583723827188, Validation Loss: 0.6931471806225411\n",
      "Epoch: 461, Training Loss: 0.6931583723827084, Validation Loss: 0.693147180622541\n",
      "Epoch: 462, Training Loss: 0.6931583723826987, Validation Loss: 0.6931471806225411\n",
      "Epoch: 463, Training Loss: 0.6931583723826887, Validation Loss: 0.6931471806225408\n",
      "Epoch: 464, Training Loss: 0.6931583723826792, Validation Loss: 0.693147180622541\n",
      "Epoch: 465, Training Loss: 0.6931583723826702, Validation Loss: 0.693147180622541\n",
      "Epoch: 466, Training Loss: 0.6931583723826614, Validation Loss: 0.693147180622541\n",
      "Epoch: 467, Training Loss: 0.6931583723826532, Validation Loss: 0.6931471806225409\n",
      "Epoch: 468, Training Loss: 0.6931583723826449, Validation Loss: 0.6931471806225411\n",
      "Epoch: 469, Training Loss: 0.6931583723826367, Validation Loss: 0.693147180622541\n",
      "Epoch: 470, Training Loss: 0.693158372382629, Validation Loss: 0.693147180622541\n",
      "Epoch: 471, Training Loss: 0.6931583723826213, Validation Loss: 0.693147180622541\n",
      "Epoch: 472, Training Loss: 0.6931583723826142, Validation Loss: 0.693147180622541\n",
      "Epoch: 473, Training Loss: 0.6931583723826069, Validation Loss: 0.693147180622541\n",
      "Epoch: 474, Training Loss: 0.6931583723826005, Validation Loss: 0.6931471806225411\n",
      "Epoch: 475, Training Loss: 0.6931583723825935, Validation Loss: 0.6931471806225409\n",
      "Epoch: 476, Training Loss: 0.6931583723825875, Validation Loss: 0.693147180622541\n",
      "Epoch: 477, Training Loss: 0.693158372382581, Validation Loss: 0.6931471806225411\n",
      "Epoch: 478, Training Loss: 0.6931583723825752, Validation Loss: 0.6931471806225409\n",
      "Epoch: 479, Training Loss: 0.6931583723825695, Validation Loss: 0.693147180622541\n",
      "Epoch: 480, Training Loss: 0.6931583723825638, Validation Loss: 0.693147180622541\n",
      "Epoch: 481, Training Loss: 0.6931583723825584, Validation Loss: 0.693147180622541\n",
      "Epoch: 482, Training Loss: 0.693158372382553, Validation Loss: 0.6931471806225409\n",
      "Epoch: 483, Training Loss: 0.6931583723825481, Validation Loss: 0.693147180622541\n",
      "Epoch: 484, Training Loss: 0.6931583723825431, Validation Loss: 0.693147180622541\n",
      "Epoch: 485, Training Loss: 0.6931583723825381, Validation Loss: 0.6931471806225411\n",
      "Epoch: 486, Training Loss: 0.6931583723825334, Validation Loss: 0.693147180622541\n",
      "Epoch: 487, Training Loss: 0.6931583723825289, Validation Loss: 0.6931471806225411\n",
      "Epoch: 488, Training Loss: 0.6931583723825245, Validation Loss: 0.6931471806225411\n",
      "Epoch: 489, Training Loss: 0.6931583723825206, Validation Loss: 0.693147180622541\n",
      "Epoch: 490, Training Loss: 0.6931583723825162, Validation Loss: 0.693147180622541\n",
      "Epoch: 491, Training Loss: 0.6931583723825122, Validation Loss: 0.693147180622541\n",
      "Epoch: 492, Training Loss: 0.6931583723825084, Validation Loss: 0.6931471806225411\n",
      "Epoch: 493, Training Loss: 0.6931583723825047, Validation Loss: 0.6931471806225411\n",
      "Epoch: 494, Training Loss: 0.6931583723825016, Validation Loss: 0.6931471806225408\n",
      "Epoch: 495, Training Loss: 0.6931583723824976, Validation Loss: 0.6931471806225411\n",
      "Epoch: 496, Training Loss: 0.6931583723824944, Validation Loss: 0.693147180622541\n",
      "Epoch: 497, Training Loss: 0.693158372382491, Validation Loss: 0.6931471806225411\n",
      "Epoch: 498, Training Loss: 0.6931583723824879, Validation Loss: 0.693147180622541\n",
      "Epoch: 499, Training Loss: 0.6931583723824851, Validation Loss: 0.6931471806225409\n",
      "Epoch: 500, Training Loss: 0.6931583723824819, Validation Loss: 0.693147180622541\n",
      "Epoch: 501, Training Loss: 0.6931583723824789, Validation Loss: 0.6931471806225409\n",
      "Epoch: 502, Training Loss: 0.6931583723824761, Validation Loss: 0.6931471806225411\n",
      "Epoch: 503, Training Loss: 0.6931583723824735, Validation Loss: 0.6931471806225411\n",
      "Epoch: 504, Training Loss: 0.6931583723824709, Validation Loss: 0.693147180622541\n",
      "Epoch: 505, Training Loss: 0.6931583723824688, Validation Loss: 0.6931471806225411\n",
      "Epoch: 506, Training Loss: 0.693158372382466, Validation Loss: 0.6931471806225408\n",
      "Epoch: 507, Training Loss: 0.6931583723824636, Validation Loss: 0.6931471806225411\n",
      "Epoch: 508, Training Loss: 0.6931583723824614, Validation Loss: 0.6931471806225409\n",
      "Epoch: 509, Training Loss: 0.6931583723824593, Validation Loss: 0.6931471806225411\n",
      "Epoch: 510, Training Loss: 0.693158372382457, Validation Loss: 0.6931471806225411\n",
      "Epoch: 511, Training Loss: 0.6931583723824548, Validation Loss: 0.6931471806225409\n",
      "Epoch: 512, Training Loss: 0.6931583723824528, Validation Loss: 0.693147180622541\n",
      "Epoch: 513, Training Loss: 0.693158372382451, Validation Loss: 0.6931471806225409\n",
      "Epoch: 514, Training Loss: 0.6931583723824488, Validation Loss: 0.693147180622541\n",
      "Epoch: 515, Training Loss: 0.6931583723824472, Validation Loss: 0.6931471806225411\n",
      "Epoch: 516, Training Loss: 0.6931583723824455, Validation Loss: 0.6931471806225409\n",
      "Epoch: 517, Training Loss: 0.6931583723824439, Validation Loss: 0.6931471806225409\n",
      "Epoch: 518, Training Loss: 0.6931583723824423, Validation Loss: 0.6931471806225411\n",
      "Epoch: 519, Training Loss: 0.6931583723824406, Validation Loss: 0.6931471806225411\n",
      "Epoch: 520, Training Loss: 0.6931583723824389, Validation Loss: 0.6931471806225411\n",
      "Epoch: 521, Training Loss: 0.6931583723824374, Validation Loss: 0.693147180622541\n",
      "Epoch: 522, Training Loss: 0.6931583723824358, Validation Loss: 0.6931471806225408\n",
      "Epoch: 523, Training Loss: 0.6931583723824346, Validation Loss: 0.6931471806225411\n",
      "Epoch: 524, Training Loss: 0.6931583723824329, Validation Loss: 0.693147180622541\n",
      "Epoch: 525, Training Loss: 0.6931583723824318, Validation Loss: 0.693147180622541\n",
      "Epoch: 526, Training Loss: 0.6931583723824308, Validation Loss: 0.6931471806225411\n",
      "Epoch: 527, Training Loss: 0.6931583723824293, Validation Loss: 0.693147180622541\n",
      "Epoch: 528, Training Loss: 0.6931583723824281, Validation Loss: 0.693147180622541\n",
      "Epoch: 529, Training Loss: 0.6931583723824268, Validation Loss: 0.6931471806225411\n",
      "Epoch: 530, Training Loss: 0.6931583723824258, Validation Loss: 0.693147180622541\n",
      "Epoch: 531, Training Loss: 0.6931583723824247, Validation Loss: 0.693147180622541\n",
      "Epoch: 532, Training Loss: 0.6931583723824235, Validation Loss: 0.6931471806225411\n",
      "Epoch: 533, Training Loss: 0.6931583723824226, Validation Loss: 0.693147180622541\n",
      "Epoch: 534, Training Loss: 0.6931583723824215, Validation Loss: 0.6931471806225408\n",
      "Epoch: 535, Training Loss: 0.6931583723824205, Validation Loss: 0.6931471806225409\n",
      "Epoch: 536, Training Loss: 0.6931583723824198, Validation Loss: 0.6931471806225411\n",
      "Epoch: 537, Training Loss: 0.6931583723824187, Validation Loss: 0.693147180622541\n",
      "Epoch: 538, Training Loss: 0.6931583723824178, Validation Loss: 0.6931471806225411\n",
      "Epoch: 539, Training Loss: 0.693158372382417, Validation Loss: 0.6931471806225409\n",
      "Epoch: 540, Training Loss: 0.6931583723824162, Validation Loss: 0.6931471806225409\n",
      "Epoch: 541, Training Loss: 0.6931583723824153, Validation Loss: 0.693147180622541\n",
      "Epoch: 542, Training Loss: 0.693158372382415, Validation Loss: 0.6931471806225411\n",
      "Epoch: 543, Training Loss: 0.6931583723824138, Validation Loss: 0.6931471806225411\n",
      "Epoch: 544, Training Loss: 0.6931583723824132, Validation Loss: 0.6931471806225411\n",
      "Epoch: 545, Training Loss: 0.6931583723824127, Validation Loss: 0.6931471806225411\n",
      "Epoch: 546, Training Loss: 0.6931583723824117, Validation Loss: 0.6931471806225411\n",
      "Epoch: 547, Training Loss: 0.6931583723824111, Validation Loss: 0.693147180622541\n",
      "Epoch: 548, Training Loss: 0.6931583723824105, Validation Loss: 0.6931471806225408\n",
      "Epoch: 549, Training Loss: 0.6931583723824098, Validation Loss: 0.6931471806225409\n",
      "Epoch: 550, Training Loss: 0.6931583723824095, Validation Loss: 0.6931471806225411\n",
      "Epoch: 551, Training Loss: 0.6931583723824091, Validation Loss: 0.6931471806225411\n",
      "Epoch: 552, Training Loss: 0.6931583723824084, Validation Loss: 0.6931471806225411\n",
      "Epoch: 553, Training Loss: 0.6931583723824079, Validation Loss: 0.6931471806225411\n",
      "Epoch: 554, Training Loss: 0.6931583723824073, Validation Loss: 0.6931471806225411\n",
      "Epoch: 555, Training Loss: 0.6931583723824067, Validation Loss: 0.6931471806225411\n",
      "Epoch: 556, Training Loss: 0.6931583723824061, Validation Loss: 0.693147180622541\n",
      "Epoch: 557, Training Loss: 0.6931583723824056, Validation Loss: 0.693147180622541\n",
      "Epoch: 558, Training Loss: 0.6931583723824052, Validation Loss: 0.6931471806225408\n",
      "Epoch: 559, Training Loss: 0.6931583723824046, Validation Loss: 0.6931471806225411\n",
      "Epoch: 560, Training Loss: 0.6931583723824045, Validation Loss: 0.6931471806225411\n",
      "Epoch: 561, Training Loss: 0.693158372382404, Validation Loss: 0.6931471806225409\n",
      "Epoch: 562, Training Loss: 0.6931583723824034, Validation Loss: 0.693147180622541\n",
      "Epoch: 563, Training Loss: 0.6931583723824031, Validation Loss: 0.6931471806225411\n",
      "Epoch: 564, Training Loss: 0.6931583723824029, Validation Loss: 0.6931471806225411\n",
      "Epoch: 565, Training Loss: 0.6931583723824024, Validation Loss: 0.693147180622541\n",
      "Epoch: 566, Training Loss: 0.693158372382402, Validation Loss: 0.6931471806225411\n",
      "Epoch: 567, Training Loss: 0.6931583723824019, Validation Loss: 0.6931471806225411\n",
      "Epoch: 568, Training Loss: 0.6931583723824013, Validation Loss: 0.6931471806225411\n",
      "Epoch: 569, Training Loss: 0.6931583723824013, Validation Loss: 0.6931471806225411\n",
      "Epoch: 570, Training Loss: 0.6931583723824006, Validation Loss: 0.6931471806225408\n",
      "Epoch: 571, Training Loss: 0.6931583723824003, Validation Loss: 0.6931471806225411\n",
      "Epoch: 572, Training Loss: 0.6931583723824001, Validation Loss: 0.6931471806225411\n",
      "Epoch: 573, Training Loss: 0.6931583723823996, Validation Loss: 0.6931471806225411\n",
      "Epoch: 574, Training Loss: 0.6931583723823993, Validation Loss: 0.6931471806225411\n",
      "Epoch: 575, Training Loss: 0.6931583723823991, Validation Loss: 0.6931471806225411\n",
      "Epoch: 576, Training Loss: 0.693158372382399, Validation Loss: 0.693147180622541\n",
      "Epoch: 577, Training Loss: 0.6931583723823986, Validation Loss: 0.6931471806225411\n",
      "Epoch: 578, Training Loss: 0.6931583723823985, Validation Loss: 0.693147180622541\n",
      "Epoch: 579, Training Loss: 0.6931583723823985, Validation Loss: 0.6931471806225411\n",
      "Epoch: 580, Training Loss: 0.693158372382398, Validation Loss: 0.693147180622541\n",
      "Epoch: 581, Training Loss: 0.693158372382398, Validation Loss: 0.6931471806225411\n",
      "Epoch: 582, Training Loss: 0.6931583723823974, Validation Loss: 0.6931471806225411\n",
      "Epoch: 583, Training Loss: 0.6931583723823973, Validation Loss: 0.693147180622541\n",
      "Epoch: 584, Training Loss: 0.6931583723823974, Validation Loss: 0.6931471806225411\n",
      "Epoch: 585, Training Loss: 0.6931583723823969, Validation Loss: 0.693147180622541\n",
      "Epoch: 586, Training Loss: 0.6931583723823967, Validation Loss: 0.693147180622541\n",
      "Epoch: 587, Training Loss: 0.6931583723823969, Validation Loss: 0.6931471806225409\n",
      "Epoch: 588, Training Loss: 0.6931583723823963, Validation Loss: 0.693147180622541\n",
      "Epoch: 589, Training Loss: 0.6931583723823962, Validation Loss: 0.6931471806225408\n",
      "Epoch: 590, Training Loss: 0.6931583723823963, Validation Loss: 0.6931471806225411\n",
      "Epoch: 591, Training Loss: 0.6931583723823957, Validation Loss: 0.6931471806225411\n",
      "Epoch: 592, Training Loss: 0.6931583723823957, Validation Loss: 0.6931471806225408\n",
      "Epoch: 593, Training Loss: 0.6931583723823957, Validation Loss: 0.6931471806225411\n",
      "Epoch: 594, Training Loss: 0.6931583723823957, Validation Loss: 0.6931471806225411\n",
      "Epoch: 595, Training Loss: 0.6931583723823953, Validation Loss: 0.693147180622541\n",
      "Epoch: 596, Training Loss: 0.6931583723823953, Validation Loss: 0.693147180622541\n",
      "Epoch: 597, Training Loss: 0.6931583723823953, Validation Loss: 0.6931471806225411\n",
      "Epoch: 598, Training Loss: 0.6931583723823951, Validation Loss: 0.6931471806225411\n",
      "Epoch: 599, Training Loss: 0.6931583723823947, Validation Loss: 0.6931471806225409\n",
      "Epoch: 600, Training Loss: 0.6931583723823947, Validation Loss: 0.693147180622541\n",
      "Epoch: 601, Training Loss: 0.6931583723823945, Validation Loss: 0.693147180622541\n",
      "Epoch: 602, Training Loss: 0.6931583723823947, Validation Loss: 0.693147180622541\n",
      "Epoch: 603, Training Loss: 0.6931583723823943, Validation Loss: 0.6931471806225411\n",
      "Epoch: 604, Training Loss: 0.6931583723823942, Validation Loss: 0.6931471806225411\n",
      "Epoch: 605, Training Loss: 0.6931583723823942, Validation Loss: 0.6931471806225411\n",
      "Epoch: 606, Training Loss: 0.693158372382394, Validation Loss: 0.6931471806225408\n",
      "Epoch: 607, Training Loss: 0.6931583723823942, Validation Loss: 0.6931471806225411\n",
      "Epoch: 608, Training Loss: 0.6931583723823942, Validation Loss: 0.6931471806225411\n",
      "Epoch: 609, Training Loss: 0.6931583723823938, Validation Loss: 0.6931471806225411\n",
      "Epoch: 610, Training Loss: 0.6931583723823936, Validation Loss: 0.693147180622541\n",
      "Epoch: 611, Training Loss: 0.6931583723823936, Validation Loss: 0.6931471806225411\n",
      "Epoch: 612, Training Loss: 0.6931583723823935, Validation Loss: 0.693147180622541\n",
      "Epoch: 613, Training Loss: 0.6931583723823936, Validation Loss: 0.6931471806225409\n",
      "Epoch: 614, Training Loss: 0.6931583723823936, Validation Loss: 0.693147180622541\n",
      "Epoch: 615, Training Loss: 0.6931583723823936, Validation Loss: 0.6931471806225411\n",
      "Epoch: 616, Training Loss: 0.6931583723823932, Validation Loss: 0.6931471806225411\n",
      "Epoch: 617, Training Loss: 0.6931583723823931, Validation Loss: 0.6931471806225411\n",
      "Epoch: 618, Training Loss: 0.6931583723823931, Validation Loss: 0.693147180622541\n",
      "Epoch: 619, Training Loss: 0.6931583723823931, Validation Loss: 0.693147180622541\n",
      "Epoch: 620, Training Loss: 0.693158372382393, Validation Loss: 0.6931471806225408\n",
      "Epoch: 621, Training Loss: 0.6931583723823931, Validation Loss: 0.6931471806225409\n",
      "Epoch: 622, Training Loss: 0.6931583723823931, Validation Loss: 0.6931471806225411\n",
      "Epoch: 623, Training Loss: 0.6931583723823931, Validation Loss: 0.6931471806225411\n",
      "Epoch: 624, Training Loss: 0.6931583723823928, Validation Loss: 0.6931471806225411\n",
      "Epoch: 625, Training Loss: 0.6931583723823926, Validation Loss: 0.6931471806225411\n",
      "Epoch: 626, Training Loss: 0.6931583723823925, Validation Loss: 0.6931471806225409\n",
      "Epoch: 627, Training Loss: 0.6931583723823925, Validation Loss: 0.693147180622541\n",
      "Epoch: 628, Training Loss: 0.6931583723823925, Validation Loss: 0.693147180622541\n",
      "Epoch: 629, Training Loss: 0.6931583723823925, Validation Loss: 0.693147180622541\n",
      "Epoch: 630, Training Loss: 0.6931583723823924, Validation Loss: 0.693147180622541\n",
      "Epoch: 631, Training Loss: 0.6931583723823924, Validation Loss: 0.693147180622541\n",
      "Epoch: 632, Training Loss: 0.6931583723823925, Validation Loss: 0.6931471806225409\n",
      "Epoch: 633, Training Loss: 0.6931583723823925, Validation Loss: 0.6931471806225411\n",
      "Epoch: 634, Training Loss: 0.6931583723823925, Validation Loss: 0.6931471806225411\n",
      "Epoch: 635, Training Loss: 0.6931583723823925, Validation Loss: 0.6931471806225411\n",
      "Epoch: 636, Training Loss: 0.6931583723823922, Validation Loss: 0.6931471806225411\n",
      "Epoch: 637, Training Loss: 0.6931583723823921, Validation Loss: 0.693147180622541\n",
      "Epoch: 638, Training Loss: 0.6931583723823921, Validation Loss: 0.6931471806225411\n",
      "Epoch: 639, Training Loss: 0.693158372382392, Validation Loss: 0.6931471806225409\n",
      "Epoch: 640, Training Loss: 0.693158372382392, Validation Loss: 0.693147180622541\n",
      "Epoch: 641, Training Loss: 0.693158372382392, Validation Loss: 0.6931471806225408\n",
      "Epoch: 642, Training Loss: 0.693158372382392, Validation Loss: 0.693147180622541\n",
      "Epoch: 643, Training Loss: 0.693158372382392, Validation Loss: 0.693147180622541\n",
      "Epoch: 644, Training Loss: 0.693158372382392, Validation Loss: 0.693147180622541\n",
      "Epoch: 645, Training Loss: 0.6931583723823919, Validation Loss: 0.6931471806225411\n",
      "Epoch: 646, Training Loss: 0.6931583723823919, Validation Loss: 0.6931471806225408\n",
      "Epoch: 647, Training Loss: 0.6931583723823919, Validation Loss: 0.693147180622541\n",
      "Epoch: 648, Training Loss: 0.693158372382392, Validation Loss: 0.6931471806225411\n",
      "Epoch: 649, Training Loss: 0.693158372382392, Validation Loss: 0.6931471806225411\n",
      "Epoch: 650, Training Loss: 0.693158372382392, Validation Loss: 0.6931471806225411\n",
      "Epoch: 651, Training Loss: 0.693158372382392, Validation Loss: 0.6931471806225411\n",
      "Epoch: 652, Training Loss: 0.693158372382392, Validation Loss: 0.6931471806225411\n",
      "Epoch: 653, Training Loss: 0.693158372382392, Validation Loss: 0.6931471806225411\n",
      "Epoch: 654, Training Loss: 0.693158372382392, Validation Loss: 0.6931471806225411\n",
      "Epoch: 655, Training Loss: 0.693158372382392, Validation Loss: 0.6931471806225411\n",
      "Epoch: 656, Training Loss: 0.6931583723823918, Validation Loss: 0.6931471806225411\n",
      "Epoch: 657, Training Loss: 0.6931583723823915, Validation Loss: 0.6931471806225411\n",
      "Epoch: 658, Training Loss: 0.6931583723823915, Validation Loss: 0.693147180622541\n",
      "Epoch: 659, Training Loss: 0.6931583723823915, Validation Loss: 0.6931471806225411\n",
      "Epoch: 660, Training Loss: 0.6931583723823915, Validation Loss: 0.6931471806225411\n",
      "Epoch: 661, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 662, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225409\n",
      "Epoch: 663, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 664, Training Loss: 0.6931583723823914, Validation Loss: 0.693147180622541\n",
      "Epoch: 665, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 666, Training Loss: 0.6931583723823914, Validation Loss: 0.693147180622541\n",
      "Epoch: 667, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225408\n",
      "Epoch: 668, Training Loss: 0.6931583723823914, Validation Loss: 0.693147180622541\n",
      "Epoch: 669, Training Loss: 0.6931583723823914, Validation Loss: 0.693147180622541\n",
      "Epoch: 670, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 671, Training Loss: 0.6931583723823914, Validation Loss: 0.693147180622541\n",
      "Epoch: 672, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225408\n",
      "Epoch: 673, Training Loss: 0.6931583723823914, Validation Loss: 0.693147180622541\n",
      "Epoch: 674, Training Loss: 0.6931583723823914, Validation Loss: 0.693147180622541\n",
      "Epoch: 675, Training Loss: 0.6931583723823913, Validation Loss: 0.693147180622541\n",
      "Epoch: 676, Training Loss: 0.6931583723823913, Validation Loss: 0.6931471806225411\n",
      "Epoch: 677, Training Loss: 0.6931583723823913, Validation Loss: 0.693147180622541\n",
      "Epoch: 678, Training Loss: 0.6931583723823913, Validation Loss: 0.6931471806225408\n",
      "Epoch: 679, Training Loss: 0.6931583723823913, Validation Loss: 0.6931471806225408\n",
      "Epoch: 680, Training Loss: 0.6931583723823913, Validation Loss: 0.693147180622541\n",
      "Epoch: 681, Training Loss: 0.6931583723823913, Validation Loss: 0.693147180622541\n",
      "Epoch: 682, Training Loss: 0.6931583723823913, Validation Loss: 0.693147180622541\n",
      "Epoch: 683, Training Loss: 0.6931583723823913, Validation Loss: 0.693147180622541\n",
      "Epoch: 684, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 685, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 686, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225409\n",
      "Epoch: 687, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225409\n",
      "Epoch: 688, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 689, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 690, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 691, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 692, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 693, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 694, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 695, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 696, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 697, Training Loss: 0.6931583723823914, Validation Loss: 0.693147180622541\n",
      "Epoch: 698, Training Loss: 0.6931583723823914, Validation Loss: 0.693147180622541\n",
      "Epoch: 699, Training Loss: 0.6931583723823914, Validation Loss: 0.693147180622541\n",
      "Epoch: 700, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 701, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 702, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 703, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 704, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 705, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 706, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 707, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 708, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 709, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 710, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 711, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 712, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 713, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 714, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225409\n",
      "Epoch: 715, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225409\n",
      "Epoch: 716, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225409\n",
      "Epoch: 717, Training Loss: 0.6931583723823913, Validation Loss: 0.6931471806225409\n",
      "Epoch: 718, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225409\n",
      "Epoch: 719, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 720, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 721, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 722, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 723, Training Loss: 0.6931583723823914, Validation Loss: 0.6931471806225411\n",
      "Epoch: 724, Training Loss: 0.6931583723823912, Validation Loss: 0.6931471806225411\n",
      "Epoch: 725, Training Loss: 0.6931583723823912, Validation Loss: 0.6931471806225411\n",
      "Epoch: 726, Training Loss: 0.6931583723823912, Validation Loss: 0.6931471806225411\n",
      "Epoch: 727, Training Loss: 0.6931583723823912, Validation Loss: 0.6931471806225411\n",
      "Epoch: 728, Training Loss: 0.6931583723823912, Validation Loss: 0.6931471806225411\n",
      "Epoch: 729, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 730, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 731, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 732, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 733, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 734, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 735, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 736, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 737, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 738, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 739, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 740, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 741, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 742, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 743, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 744, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 745, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 746, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 747, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 748, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 749, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 750, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 751, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 752, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 753, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 754, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 755, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 756, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 757, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 758, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 759, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 760, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 761, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 762, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 763, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 764, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 765, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 766, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 767, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 768, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 769, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 770, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 771, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 772, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 773, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 774, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 775, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 776, Training Loss: 0.693158372382391, Validation Loss: 0.693147180622541\n",
      "Epoch: 777, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 778, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 779, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 780, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 781, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 782, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 783, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 784, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 785, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 786, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 787, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 788, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 789, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 790, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 791, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 792, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 793, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 794, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 795, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 796, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 797, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 798, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 799, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 800, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 801, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 802, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 803, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 804, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 805, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 806, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 807, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 808, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 809, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 810, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 811, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 812, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 813, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 814, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 815, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 816, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 817, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 818, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 819, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 820, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 821, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 822, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 823, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 824, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 825, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 826, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 827, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 828, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 829, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 830, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 831, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 832, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 833, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 834, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 835, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 836, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 837, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 838, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 839, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 840, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 841, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 842, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 843, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 844, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 845, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 846, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 847, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 848, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 849, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 850, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 851, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 852, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 853, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 854, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 855, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 856, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 857, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 858, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 859, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 860, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 861, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 862, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 863, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 864, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 865, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 866, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 867, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 868, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 869, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 870, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 871, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 872, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 873, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 874, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 875, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 876, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 877, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 878, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 879, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 880, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 881, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 882, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 883, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 884, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 885, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 886, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 887, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 888, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 889, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 890, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 891, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 892, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 893, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 894, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 895, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 896, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 897, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 898, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 899, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 900, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 901, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 902, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 903, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 904, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 905, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 906, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 907, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 908, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 909, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 910, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 911, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 912, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 913, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 914, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 915, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 916, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 917, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 918, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 919, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 920, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 921, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 922, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 923, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 924, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 925, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 926, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 927, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 928, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 929, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 930, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 931, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 932, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 933, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 934, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 935, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 936, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 937, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 938, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 939, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 940, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 941, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 942, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 943, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 944, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 945, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 946, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 947, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 948, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 949, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 950, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 951, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 952, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 953, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 954, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 955, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 956, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 957, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 958, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 959, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 960, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 961, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 962, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 963, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 964, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 965, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 966, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 967, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 968, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 969, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 970, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 971, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 972, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 973, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 974, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 975, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 976, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 977, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 978, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 979, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 980, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 981, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 982, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 983, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 984, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 985, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 986, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 987, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 988, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 989, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 990, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 991, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 992, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 993, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 994, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 995, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 996, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 997, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 998, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n",
      "Epoch: 999, Training Loss: 0.693158372382391, Validation Loss: 0.6931471806225411\n"
     ]
    }
   ],
   "source": [
    "# Lists to store training and validation losses\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(train_X), batch_size):\n",
    "        # Mini-batch\n",
    "        X_batch = train_X[i : i + batch_size]\n",
    "        y_batch = train_y_one_hot[i : i + batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        layer_0 = X_batch\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0) + bias_0)\n",
    "        layer_2 = relu(np.dot(layer_1, weights_1) + bias_1)\n",
    "        output = sigmoid(np.dot(layer_2, weights_2) + bias_2)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = binary_cross_entropy_loss(y_batch, output)\n",
    "\n",
    "        # Backward pass\n",
    "        d_output = output - y_batch\n",
    "        d_layer_2 = np.dot(d_output, weights_2.T) * (layer_2 > 0)\n",
    "        d_layer_1 = np.dot(d_layer_2, weights_1.T) * (layer_1 > 0)\n",
    "\n",
    "        # Update weights and biases\n",
    "        weights_2 -= learning_rate * np.dot(layer_2.T, d_output) / batch_size\n",
    "        bias_2 -= learning_rate * np.sum(d_output, axis=0, keepdims=True) / batch_size\n",
    "\n",
    "        weights_1 -= learning_rate * np.dot(layer_1.T, d_layer_2) / batch_size\n",
    "        bias_1 -= learning_rate * np.sum(d_layer_2, axis=0, keepdims=True) / batch_size\n",
    "\n",
    "        weights_0 -= learning_rate * np.dot(layer_0.T, d_layer_1) / batch_size\n",
    "        bias_0 -= learning_rate * np.sum(d_layer_1, axis=0, keepdims=True) / batch_size\n",
    "\n",
    "    # Calculate validation loss\n",
    "    layer_1_val = relu(np.dot(validate_X, weights_0) + bias_0)\n",
    "    layer_2_val = relu(np.dot(layer_1_val, weights_1) + bias_1)\n",
    "    output_val = sigmoid(np.dot(layer_2_val, weights_2) + bias_2)\n",
    "    val_loss = binary_cross_entropy_loss(validate_y_one_hot, output_val)\n",
    "\n",
    "    # Print loss and check for early stopping\n",
    "    print(f\"Epoch: {epoch}, Training Loss: {loss}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Store losses for plotting\n",
    "    training_losses.append(loss)\n",
    "    validation_losses.append(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.5\n",
      "Confusion Matrix:\n",
      "[[20.49977063 20.50022937]\n",
      " [20.49977063 20.50022937]]\n"
     ]
    }
   ],
   "source": [
    "# Plot the loss over epochs\n",
    "plt.plot(training_losses, label=\"Training Loss\")\n",
    "plt.plot(validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss over Epochs for Zero Parameter Initialization\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Final accuracy and confusion matrix on the validation set\n",
    "predicted_labels = np.argmax(output_val, axis=1)\n",
    "accuracy = np.mean(predicted_labels == validate_y.flatten())\n",
    "confusion_matrix = np.dot(np.transpose(validate_y_one_hot), output_val)\n",
    "print(f\"Final Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
